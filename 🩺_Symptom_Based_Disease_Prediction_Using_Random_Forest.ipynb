{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm7Rn9YlvGLvq0zuerfPac",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IbrahimJenberu/Social-Media-Sentimental-Analysis-ML-/blob/main/%F0%9F%A9%BA_Symptom_Based_Disease_Prediction_Using_Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LjH6lLVeNEA",
        "outputId": "29f187b1-d5e0-4dd6-faa4-f09f4b6445d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.14.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost joblib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import random\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def generate_synthetic_data(n_samples=2500):\n",
        "    \"\"\"\n",
        "    Generate synthetic Ethiopian patient data for disease prediction\n",
        "\n",
        "    Args:\n",
        "        n_samples: Number of patient records to generate\n",
        "\n",
        "    Returns:\n",
        "        DataFrame containing synthetic patient data\n",
        "    \"\"\"\n",
        "    print(f\"Generating {n_samples} synthetic patient records...\")\n",
        "\n",
        "    # Ethiopian regions\n",
        "    regions = ['Addis Ababa', 'Oromia', 'Amhara', 'Tigray', 'SNNPR',\n",
        "               'Somali', 'Afar', 'Benishangul-Gumuz', 'Gambela', 'Harari', 'Dire Dawa']\n",
        "\n",
        "    # Common symptoms in Ethiopia\n",
        "    symptoms = ['fever', 'cough', 'headache', 'fatigue', 'nausea', 'vomiting',\n",
        "                'diarrhea', 'abdominal_pain', 'chest_pain', 'difficulty_breathing',\n",
        "                'joint_pain', 'rash', 'sore_throat', 'chills', 'loss_of_appetite',\n",
        "                'jaundice', 'swelling', 'blood_in_stool', 'night_sweats', 'weight_loss']\n",
        "\n",
        "    # Pre-existing conditions common in Ethiopia\n",
        "    conditions = ['hypertension', 'diabetes', 'HIV', 'tuberculosis', 'malaria',\n",
        "                  'hepatitis', 'anemia', 'asthma', 'heart_disease', 'malnutrition']\n",
        "\n",
        "    # Common diseases in Ethiopia (target variable)\n",
        "    diseases = ['Malaria', 'Tuberculosis', 'Typhoid Fever', 'Pneumonia',\n",
        "                'Diarrheal Disease', 'HIV/AIDS', 'Acute Respiratory Infection',\n",
        "                'Intestinal Parasites', 'Hepatitis', 'Meningitis']\n",
        "\n",
        "    # Disease severity mapping (used for creating correlation between symptoms and disease)\n",
        "    disease_severity = {\n",
        "        'Malaria': 65,\n",
        "        'Tuberculosis': 70,\n",
        "        'Typhoid Fever': 60,\n",
        "        'Pneumonia': 68,\n",
        "        'Diarrheal Disease': 55,\n",
        "        'HIV/AIDS': 75,\n",
        "        'Acute Respiratory Infection': 58,\n",
        "        'Intestinal Parasites': 50,\n",
        "        'Hepatitis': 63,\n",
        "        'Meningitis': 72\n",
        "    }\n",
        "\n",
        "    # Symptom-disease correlation matrix (simplified)\n",
        "    symptom_disease_corr = {\n",
        "        'Malaria': ['fever', 'chills', 'headache', 'fatigue', 'nausea'],\n",
        "        'Tuberculosis': ['cough', 'chest_pain', 'difficulty_breathing', 'night_sweats', 'weight_loss'],\n",
        "        'Typhoid Fever': ['fever', 'headache', 'fatigue', 'abdominal_pain', 'loss_of_appetite'],\n",
        "        'Pneumonia': ['cough', 'fever', 'difficulty_breathing', 'chest_pain', 'chills'],\n",
        "        'Diarrheal Disease': ['diarrhea', 'vomiting', 'abdominal_pain', 'fever', 'dehydration'],\n",
        "        'HIV/AIDS': ['weight_loss', 'fever', 'night_sweats', 'fatigue', 'rash'],\n",
        "        'Acute Respiratory Infection': ['cough', 'sore_throat', 'fever', 'difficulty_breathing', 'fatigue'],\n",
        "        'Intestinal Parasites': ['abdominal_pain', 'diarrhea', 'weight_loss', 'fatigue', 'loss_of_appetite'],\n",
        "        'Hepatitis': ['jaundice', 'fatigue', 'nausea', 'abdominal_pain', 'loss_of_appetite'],\n",
        "        'Meningitis': ['headache', 'fever', 'neck_stiffness', 'vomiting', 'sensitivity_to_light']\n",
        "    }\n",
        "\n",
        "    # Initialize empty dataframe\n",
        "    df = pd.DataFrame()\n",
        "\n",
        "    # Generate data in batches for memory efficiency\n",
        "    batch_size = min(10000, n_samples)\n",
        "    remaining = n_samples\n",
        "\n",
        "    while remaining > 0:\n",
        "        current_batch = min(batch_size, remaining)\n",
        "\n",
        "        # Initialize data dictionary for this batch\n",
        "        data = {\n",
        "            'patient_id': [f'ETH{i:06d}' for i in range(n_samples - remaining + 1, n_samples - remaining + current_batch + 1)],\n",
        "            'age': np.random.randint(0, 101, size=current_batch),\n",
        "            'gender': np.random.choice(['male', 'female'], size=current_batch),\n",
        "            'region': np.random.choice(regions, size=current_batch),\n",
        "            'symptom_duration_days': np.random.randint(1, 31, size=current_batch),\n",
        "            'severity_level': np.random.choice(['mild', 'moderate', 'severe'], size=current_batch),\n",
        "        }\n",
        "\n",
        "        # Generate symptom data (binary) - vectorized for speed\n",
        "        for symptom in symptoms:\n",
        "            data[f'symptom_{symptom}'] = np.random.choice([0, 1], size=current_batch, p=[0.7, 0.3])\n",
        "\n",
        "        # Generate pre-existing conditions\n",
        "        num_conditions = np.random.choice([0, 1, 2, 3], size=current_batch, p=[0.5, 0.3, 0.15, 0.05])\n",
        "\n",
        "        # Vectorized approach to generate conditions\n",
        "        data['pre_existing_conditions'] = [''] * current_batch\n",
        "        for i in range(current_batch):\n",
        "            if num_conditions[i] > 0:\n",
        "                data['pre_existing_conditions'][i] = ','.join(random.sample(conditions, num_conditions[i]))\n",
        "\n",
        "        # Create batch dataframe\n",
        "        batch_df = pd.DataFrame(data)\n",
        "\n",
        "        # Generate disease based on symptoms (with some correlation)\n",
        "        batch_df['diagnosed_disease'] = ''\n",
        "        batch_df['disease_severity_score'] = 0\n",
        "\n",
        "        # More efficient approach to disease assignment\n",
        "        symptom_scores = np.zeros((current_batch, len(diseases)))\n",
        "\n",
        "        # Calculate symptom scores for each disease\n",
        "        for d_idx, disease in enumerate(diseases):\n",
        "            for symptom in symptom_disease_corr.get(disease, []):\n",
        "                symptom_col = f'symptom_{symptom}'\n",
        "                if symptom_col in batch_df.columns:\n",
        "                    symptom_scores[:, d_idx] += batch_df[symptom_col].values\n",
        "\n",
        "        # Add severity factor\n",
        "        severity_factor = np.zeros(current_batch)\n",
        "        severity_factor[batch_df['severity_level'] == 'moderate'] = 0.5\n",
        "        severity_factor[batch_df['severity_level'] == 'severe'] = 1.0\n",
        "\n",
        "        for d_idx in range(len(diseases)):\n",
        "            symptom_scores[:, d_idx] += severity_factor\n",
        "\n",
        "        # Add duration factor\n",
        "        duration_factor = (batch_df['symptom_duration_days'] > 14).astype(float) * 0.5\n",
        "        for d_idx in range(len(diseases)):\n",
        "            symptom_scores[:, d_idx] += duration_factor\n",
        "\n",
        "        # Add randomness\n",
        "        symptom_scores += np.random.uniform(0, 0.2, size=symptom_scores.shape)\n",
        "\n",
        "        # Get index of disease with highest score\n",
        "        disease_indices = np.argmax(symptom_scores, axis=1)\n",
        "\n",
        "        # Assign diseases\n",
        "        batch_df['diagnosed_disease'] = [diseases[i] for i in disease_indices]\n",
        "\n",
        "        # Calculate severity scores\n",
        "        base_scores = np.array([disease_severity[d] for d in batch_df['diagnosed_disease']])\n",
        "        symptom_count = batch_df[[c for c in batch_df.columns if c.startswith('symptom_')]].sum(axis=1).values\n",
        "        condition_count = batch_df['pre_existing_conditions'].apply(lambda x: len(x.split(',')) if x else 0).values\n",
        "\n",
        "        # Age factor\n",
        "        age_factor = np.zeros(current_batch)\n",
        "        age_factor[(batch_df['age'] < 5) | (batch_df['age'] > 65)] = 5\n",
        "\n",
        "        # Final score calculation\n",
        "        scores = base_scores + (symptom_count * 1.5) + (condition_count * 3) + age_factor\n",
        "        scores += np.random.normal(0, 5, size=current_batch)\n",
        "\n",
        "        # Clip scores\n",
        "        scores = np.clip(scores, 0, 100)\n",
        "        batch_df['disease_severity_score'] = scores\n",
        "\n",
        "        # Append batch to main dataframe\n",
        "        df = pd.concat([df, batch_df], ignore_index=True)\n",
        "\n",
        "        # Update remaining count\n",
        "        remaining -= current_batch\n",
        "\n",
        "    print(f\"Generated {n_samples} patient records with {len(symptoms)} symptoms\")\n",
        "    return df\n",
        "\n",
        "def perform_eda(df):\n",
        "    \"\"\"\n",
        "    Perform exploratory data analysis on patient data\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing patient data\n",
        "    \"\"\"\n",
        "    print(\"Performing exploratory data analysis...\")\n",
        "\n",
        "    # Basic statistics - more selective to improve performance\n",
        "    numeric_cols = ['age', 'symptom_duration_days', 'disease_severity_score']\n",
        "    print(\"\\nBasic statistics (numeric features):\")\n",
        "    print(df[numeric_cols].describe().T)\n",
        "\n",
        "    # Disease distribution\n",
        "    print(\"\\nDisease distribution:\")\n",
        "    disease_counts = df['diagnosed_disease'].value_counts()\n",
        "    print(disease_counts)\n",
        "\n",
        "    # More efficient plotting\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    # Only plot top 10 diseases for clarity\n",
        "    top_diseases = disease_counts.nlargest(10)\n",
        "    sns.barplot(x=top_diseases.values, y=top_diseases.index)\n",
        "    plt.title('Distribution of Top 10 Diagnosed Diseases')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('disease_distribution.png')\n",
        "    plt.close()  # Close figure to save memory\n",
        "\n",
        "    # Severity score distribution\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.histplot(df['disease_severity_score'], bins=20, kde=True)\n",
        "    plt.title('Distribution of Disease Severity Scores')\n",
        "    plt.savefig('severity_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Age distribution by gender - more efficient\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Use violin plot for better distribution visualization\n",
        "    sns.violinplot(x='gender', y='age', data=df)\n",
        "    plt.title('Age Distribution by Gender')\n",
        "    plt.savefig('age_gender_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Correlation between symptoms and severity - vectorized approach\n",
        "    symptom_cols = [col for col in df.columns if col.startswith('symptom_')]\n",
        "\n",
        "    # Calculate correlations efficiently\n",
        "    corr_series = pd.Series(index=symptom_cols)\n",
        "    for col in symptom_cols:\n",
        "        corr_series[col] = df[col].corr(df['disease_severity_score'])\n",
        "\n",
        "    # Sort by correlation\n",
        "    corr_series = corr_series.sort_values(ascending=False)\n",
        "\n",
        "    # Plot top 15 symptoms for clarity\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.barplot(x=corr_series.iloc[:15].values, y=corr_series.iloc[:15].index)\n",
        "    plt.title('Top 15 Symptoms Correlated with Disease Severity')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('symptom_correlations.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Just use basic numeric columns for correlation heatmap\n",
        "    # Avoid using condition_count and symptom_count which don't exist yet\n",
        "    numeric_cols = ['age', 'symptom_duration_days', 'disease_severity_score']\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Matrix of Numeric Features')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"EDA complete. Visualizations saved as PNG files.\")\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess the patient data for model training\n",
        "\n",
        "    Args:\n",
        "        df: DataFrame containing patient data\n",
        "\n",
        "    Returns:\n",
        "        Processed X features and y target\n",
        "    \"\"\"\n",
        "    print(\"Preprocessing data...\")\n",
        "\n",
        "    # Make a copy to avoid modifying the original\n",
        "    processed_df = df.copy()\n",
        "\n",
        "    # Handle missing values in numeric columns - use more robust approaches\n",
        "    processed_df['age'] = processed_df['age'].fillna(processed_df['age'].median())\n",
        "    processed_df['symptom_duration_days'] = processed_df['symptom_duration_days'].fillna(processed_df['symptom_duration_days'].median())\n",
        "\n",
        "    # Create symptom feature matrix\n",
        "    symptom_cols = [col for col in processed_df.columns if col.startswith('symptom_')]\n",
        "\n",
        "    # Check for and drop any duplicate columns before creating new ones\n",
        "    processed_df = processed_df.loc[:, ~processed_df.columns.duplicated()]\n",
        "\n",
        "    # Extract pre-existing conditions and count them more efficiently\n",
        "    processed_df['condition_count'] = processed_df['pre_existing_conditions'].str.count(',') + processed_df['pre_existing_conditions'].astype(bool)\n",
        "\n",
        "    # Create aggregate symptom features\n",
        "    processed_df['symptom_count'] = processed_df[symptom_cols].sum(axis=1)\n",
        "    processed_df['symptom_density'] = processed_df['symptom_count'] / processed_df['symptom_duration_days'].clip(lower=1)\n",
        "\n",
        "    # Add interaction features - Age and severity\n",
        "    processed_df['age_severity_interaction'] = 0\n",
        "    mask_high_risk = (processed_df['age'] < 5) | (processed_df['age'] > 65)\n",
        "    mask_severe = processed_df['severity_level'] == 'severe'\n",
        "    processed_df.loc[mask_high_risk & mask_severe, 'age_severity_interaction'] = 1\n",
        "\n",
        "    # Prepare features for modeling\n",
        "    cat_features = ['gender', 'region', 'severity_level']\n",
        "    num_features = ['age', 'symptom_duration_days', 'condition_count',\n",
        "                   'symptom_count', 'symptom_density', 'age_severity_interaction']\n",
        "\n",
        "    # Check if we have all the features needed - fail early if not\n",
        "    for feature in cat_features + num_features:\n",
        "        if feature not in processed_df.columns:\n",
        "            raise ValueError(f\"Required feature '{feature}' is missing from the DataFrame\")\n",
        "\n",
        "    # Target variable is the disease severity score\n",
        "    y = processed_df['disease_severity_score'].values\n",
        "\n",
        "    # Combine all features\n",
        "    features = cat_features + num_features + symptom_cols\n",
        "\n",
        "    # Verify all features exist and get unique columns only\n",
        "    X = processed_df[features].copy()\n",
        "\n",
        "    # Final check for duplicated columns before returning\n",
        "    if X.columns.duplicated().any():\n",
        "        # Drop duplicate columns if they exist\n",
        "        X = X.loc[:, ~X.columns.duplicated()]\n",
        "        print(\"Warning: Duplicated columns were removed from the feature set\")\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def build_model(X, y):\n",
        "    \"\"\"\n",
        "    Build and train a regression model for disease severity prediction\n",
        "\n",
        "    Args:\n",
        "        X: Feature matrix\n",
        "        y: Target variable\n",
        "\n",
        "    Returns:\n",
        "        Trained model and preprocessor\n",
        "    \"\"\"\n",
        "    print(\"Building and training model...\")\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Verify columns in feature matrix\n",
        "    print(f\"\\nFeature columns: {X.columns.tolist()}\")\n",
        "    print(f\"Number of features: {len(X.columns)}\")\n",
        "    print(f\"Any duplicate columns: {any(X.columns.duplicated())}\")\n",
        "\n",
        "    # Create preprocessor\n",
        "    categorical_features = ['gender', 'region', 'severity_level']\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Set sparse=False for better compatibility\n",
        "    ])\n",
        "\n",
        "    # Ensure we're using only the numeric features that exist in the DataFrame\n",
        "    numerical_features = [col for col in ['age', 'symptom_duration_days', 'condition_count',\n",
        "                          'symptom_count', 'symptom_density', 'age_severity_interaction'] if col in X.columns]\n",
        "\n",
        "    numerical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    symptom_features = [col for col in X.columns if col.startswith('symptom_')]\n",
        "    symptom_transformer = 'passthrough'  # Binary features don't need transformation\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features),\n",
        "            ('sym', symptom_transformer, symptom_features)\n",
        "        ],\n",
        "        remainder='drop'  # Drop any columns not specified\n",
        "    )\n",
        "\n",
        "    # Define models to try with optimized defaults\n",
        "    models = {\n",
        "        'RandomForest': RandomForestRegressor(random_state=42, n_jobs=-1),  # Use all cores\n",
        "        'GradientBoosting': GradientBoostingRegressor(random_state=42),\n",
        "        'XGBoost': XGBRegressor(random_state=42, n_jobs=-1)  # Use all cores\n",
        "    }\n",
        "\n",
        "    # Optimized hyperparameter grids - smaller for faster execution\n",
        "    param_grids = {\n",
        "        'RandomForest': {\n",
        "            'model__n_estimators': [100],\n",
        "            'model__max_depth': [None, 20],\n",
        "            'model__min_samples_split': [2, 5]\n",
        "        },\n",
        "        'GradientBoosting': {\n",
        "            'model__n_estimators': [100],\n",
        "            'model__learning_rate': [0.05, 0.1]\n",
        "        },\n",
        "        'XGBoost': {\n",
        "            'model__n_estimators': [100],\n",
        "            'model__learning_rate': [0.05, 0.1],\n",
        "            'model__max_depth': [5, 7]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Train and evaluate each model\n",
        "    best_score = 0\n",
        "    best_model_name = None\n",
        "    best_model = None\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "        # Create pipeline with preprocessor and model\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('model', model)\n",
        "        ])\n",
        "\n",
        "        # Use grid search for hyperparameter tuning\n",
        "        grid_search = GridSearchCV(\n",
        "            pipeline,\n",
        "            param_grids[model_name],\n",
        "            cv=5,\n",
        "            scoring='r2',\n",
        "            n_jobs=-1,  # Use all cores\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Fit model\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Get best model\n",
        "        y_pred = grid_search.predict(X_test)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "        print(f\"{model_name} - Best parameters: {grid_search.best_params_}\")\n",
        "        print(f\"{model_name} - R² Score: {r2:.4f}\")\n",
        "        print(f\"{model_name} - MAE: {mae:.4f}\")\n",
        "        print(f\"{model_name} - RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # Keep track of best model\n",
        "        if r2 > best_score:\n",
        "            best_score = r2\n",
        "            best_model_name = model_name\n",
        "            best_model = grid_search.best_estimator_\n",
        "\n",
        "    print(f\"\\nBest model: {best_model_name} with R² score of {best_score:.4f}\")\n",
        "\n",
        "    # Evaluate with cross-validation\n",
        "    cv_scores = cross_val_score(best_model, X, y, cv=5, scoring='r2', n_jobs=-1)\n",
        "    print(f\"Cross-validation R² scores: {cv_scores}\")\n",
        "    print(f\"Mean CV R² score: {np.mean(cv_scores):.4f}\")\n",
        "\n",
        "    # Extract preprocessor from pipeline for separate saving\n",
        "    preprocessor = best_model.named_steps['preprocessor']\n",
        "\n",
        "    return best_model, preprocessor, best_model_name\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    \"\"\"\n",
        "    Evaluate the trained model and visualize results\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X: Feature matrix\n",
        "        y: Target variable\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating model performance...\")\n",
        "\n",
        "    # Make predictions in batches for memory efficiency\n",
        "    batch_size = 1000\n",
        "    n_samples = len(X)\n",
        "    y_pred = np.zeros(n_samples)\n",
        "\n",
        "    for i in range(0, n_samples, batch_size):\n",
        "        end = min(i + batch_size, n_samples)\n",
        "        y_pred[i:end] = model.predict(X.iloc[i:end])\n",
        "\n",
        "    # Calculate metrics\n",
        "    r2 = r2_score(y, y_pred)\n",
        "    mae = mean_absolute_error(y, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
        "\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "    print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "\n",
        "    # Calculate accuracy as percentage of predictions within ±5 points\n",
        "    within_5_points = np.mean(np.abs(y - y_pred) <= 5)\n",
        "    within_10_points = np.mean(np.abs(y - y_pred) <= 10)\n",
        "    print(f\"Accuracy (predictions within ±5 points): {within_5_points:.4f} ({within_5_points*100:.2f}%)\")\n",
        "    print(f\"Accuracy (predictions within ±10 points): {within_10_points:.4f} ({within_10_points*100:.2f}%)\")\n",
        "\n",
        "    # Visualize actual vs predicted values - use sample for large datasets\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # If dataset is large, sample for visualization\n",
        "    if len(y) > 1000:\n",
        "        sample_indices = np.random.choice(range(len(y)), 1000, replace=False)\n",
        "        y_sample = y[sample_indices]\n",
        "        y_pred_sample = y_pred[sample_indices]\n",
        "        plt.scatter(y_sample, y_pred_sample, alpha=0.5)\n",
        "    else:\n",
        "        plt.scatter(y, y_pred, alpha=0.5)\n",
        "\n",
        "    # Plot reference line\n",
        "    min_val = min(np.min(y), np.min(y_pred))\n",
        "    max_val = max(np.max(y), np.max(y_pred))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "    plt.xlabel('Actual Severity Score')\n",
        "    plt.ylabel('Predicted Severity Score')\n",
        "    plt.title('Actual vs. Predicted Severity Scores')\n",
        "    plt.savefig('actual_vs_predicted.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Visualize residuals\n",
        "    residuals = y - y_pred\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # If dataset is large, sample for visualization\n",
        "    if len(y) > 1000:\n",
        "        plt.scatter(y_pred[sample_indices], residuals[sample_indices], alpha=0.5)\n",
        "    else:\n",
        "        plt.scatter(y_pred, residuals, alpha=0.5)\n",
        "\n",
        "    plt.axhline(y=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Predicted Severity Score')\n",
        "    plt.ylabel('Residuals (Actual - Predicted)')\n",
        "    plt.title('Residual Plot')\n",
        "    plt.savefig('residuals.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Histogram of residuals\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(residuals, bins=30, alpha=0.7)\n",
        "    plt.axvline(x=0, color='r', linestyle='--')\n",
        "    plt.xlabel('Residual Value')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Histogram of Residuals')\n",
        "    plt.savefig('residuals_histogram.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Feature importance for tree-based models\n",
        "    if hasattr(model, 'named_steps') and hasattr(model.named_steps['model'], 'feature_importances_'):\n",
        "        # Get feature names after preprocessing\n",
        "        preprocessor = model.named_steps['preprocessor']\n",
        "        features = []\n",
        "\n",
        "        # Get feature names from each transformer\n",
        "        if hasattr(preprocessor, 'transformers_'):\n",
        "            for name, transformer, columns in preprocessor.transformers_:\n",
        "                if name == 'cat' and hasattr(transformer, 'named_steps') and 'onehot' in transformer.named_steps:\n",
        "                    encoder = transformer.named_steps['onehot']\n",
        "                    if hasattr(encoder, 'get_feature_names_out'):\n",
        "                        encoded_features = encoder.get_feature_names_out(columns)\n",
        "                        features.extend(encoded_features)\n",
        "                elif name == 'num':\n",
        "                    features.extend(columns)\n",
        "                elif name == 'sym':\n",
        "                    features.extend(columns)\n",
        "\n",
        "        # Get feature importances\n",
        "        importances = model.named_steps['model'].feature_importances_\n",
        "\n",
        "        # If we have the right number of feature names\n",
        "        if len(features) == len(importances):\n",
        "            # Create DataFrame of features and importances\n",
        "            feature_importance = pd.DataFrame({'feature': features, 'importance': importances})\n",
        "            feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
        "\n",
        "            # Plot top 20 features\n",
        "            plt.figure(figsize=(12, 8))\n",
        "            sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
        "            plt.title('Top 20 Feature Importances')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig('feature_importance.png')\n",
        "            plt.close()\n",
        "\n",
        "            # Print top 10 features\n",
        "            print(\"\\nTop 10 important features:\")\n",
        "            print(feature_importance.head(10))\n",
        "\n",
        "    print(\"Model evaluation complete. Visualizations saved as PNG files.\")\n",
        "\n",
        "def save_model(model, preprocessor, model_name):\n",
        "    \"\"\"\n",
        "    Save the trained model and preprocessor for later use\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        preprocessor: Data preprocessor\n",
        "        model_name: Name of the model\n",
        "    \"\"\"\n",
        "    print(\"\\nSaving model and preprocessor...\")\n",
        "\n",
        "    # Create timestamp for model version\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Save the full pipeline with compression for smaller file size\n",
        "    model_filename = f\"ethiopian_disease_model_{model_name}_{timestamp}.pkl\"\n",
        "    joblib.dump(model, model_filename, compress=3)\n",
        "\n",
        "    # Save preprocessor separately\n",
        "    preprocessor_filename = f\"ethiopian_disease_preprocessor_{timestamp}.pkl\"\n",
        "    joblib.dump(preprocessor, preprocessor_filename, compress=3)\n",
        "\n",
        "    print(f\"Model saved as: {model_filename}\")\n",
        "    print(f\"Preprocessor saved as: {preprocessor_filename}\")\n",
        "\n",
        "    return model_filename, preprocessor_filename\n",
        "\n",
        "def create_inference_function(model_filename, preprocessor_filename):\n",
        "    \"\"\"\n",
        "    Create a function for inference with the trained model\n",
        "\n",
        "    Args:\n",
        "        model_filename: Filename of the saved model\n",
        "        preprocessor_filename: Filename of the saved preprocessor\n",
        "\n",
        "    Returns:\n",
        "        Inference function\n",
        "    \"\"\"\n",
        "    # Load the model and preprocessor\n",
        "    model = joblib.load(model_filename)\n",
        "\n",
        "    # Create a more efficient prediction function using model closure\n",
        "    def predict_disease_severity(patient_data):\n",
        "        \"\"\"\n",
        "        Predict disease severity for a new patient\n",
        "\n",
        "        Args:\n",
        "            patient_data: Dictionary containing patient information\n",
        "                Required keys: age, gender, region, symptom_*, severity_level,\n",
        "                symptom_duration_days, pre_existing_conditions\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with severity score and risk level\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Convert to DataFrame\n",
        "            patient_df = pd.DataFrame([patient_data])\n",
        "\n",
        "            # Add derived features needed by the model\n",
        "            # Count pre-existing conditions\n",
        "            patient_df['condition_count'] = patient_df['pre_existing_conditions'].apply(\n",
        "                lambda x: len(x.split(',')) if isinstance(x, str) and x else 0\n",
        "            )\n",
        "\n",
        "            # Count symptoms\n",
        "            symptom_cols = [col for col in patient_df.columns if col.startswith('symptom_')]\n",
        "            patient_df['symptom_count'] = patient_df[symptom_cols].sum(axis=1)\n",
        "            patient_df['symptom_density'] = patient_df['symptom_count'] / patient_df['symptom_duration_days'].clip(lower=1)\n",
        "\n",
        "            # Age severity interaction\n",
        "            patient_df['age_severity_interaction'] = 0\n",
        "            age = patient_df['age'].iloc[0]\n",
        "            severity = patient_df['severity_level'].iloc[0]\n",
        "            if (age < 5 or age > 65) and severity == 'severe':\n",
        "                patient_df.loc[0, 'age_severity_interaction'] = 1\n",
        "\n",
        "            # Make prediction using the full pipeline\n",
        "            severity_score = model.predict(patient_df)[0]\n",
        "\n",
        "            # Map severity score to disease risk level\n",
        "            if severity_score < 40:\n",
        "                risk_level = \"Low\"\n",
        "            elif severity_score < 60:\n",
        "                risk_level = \"Moderate\"\n",
        "            elif severity_score < 80:\n",
        "                risk_level = \"High\"\n",
        "            else:\n",
        "                risk_level = \"Severe\"\n",
        "\n",
        "            return {\n",
        "                'severity_score': round(severity_score, 2),\n",
        "                'risk_level': risk_level\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'error': str(e),\n",
        "                'severity_score': None,\n",
        "                'risk_level': None\n",
        "            }\n",
        "\n",
        "    print(\"\\nInference function created successfully.\")\n",
        "\n",
        "    # Example usage\n",
        "    print(\"\\nExample inference usage:\")\n",
        "    example_patient = {\n",
        "        'age': 35,\n",
        "        'gender': 'female',\n",
        "        'region': 'Addis Ababa',\n",
        "        'symptom_fever': 1,\n",
        "        'symptom_cough': 1,\n",
        "        'symptom_headache': 0,\n",
        "        'symptom_fatigue': 1,\n",
        "        'symptom_nausea': 0,\n",
        "        'symptom_vomiting': 0,\n",
        "        'symptom_diarrhea': 0,\n",
        "        'symptom_abdominal_pain': 0,\n",
        "        'symptom_chest_pain': 1,\n",
        "        'symptom_difficulty_breathing': 1,\n",
        "        'symptom_joint_pain': 0,\n",
        "        'symptom_rash': 0,\n",
        "        'symptom_sore_throat': 1,\n",
        "        'symptom_chills': 1,\n",
        "        'symptom_loss_of_appetite': 0,\n",
        "        'symptom_jaundice': 0,\n",
        "        'symptom_swelling': 0,\n",
        "        'symptom_blood_in_stool': 0,\n",
        "        'symptom_night_sweats': 0,\n",
        "        'symptom_weight_loss': 0,\n",
        "        'symptom_duration_days': 5,\n",
        "        'severity_level': 'moderate',\n",
        "        'pre_existing_conditions': 'asthma'\n",
        "    }\n",
        "\n",
        "    result = predict_disease_severity(example_patient)\n",
        "    print(f\"Example Patient - Predicted Severity Score: {result['severity_score']}\")\n",
        "    print(f\"Example Patient - Risk Level: {result['risk_level']}\")\n",
        "\n",
        "    return predict_disease_severity\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the entire pipeline\n",
        "    \"\"\"\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Starting Ethiopian Disease Prediction Pipeline at {start_time}\")\n",
        "    print(\"=============================================\")\n",
        "\n",
        "    # Step 1: Generate synthetic data\n",
        "    df = generate_synthetic_data(n_samples=2500)\n",
        "\n",
        "    # Step 2: Perform EDA\n",
        "    perform_eda(df)\n",
        "\n",
        "    # Step 3: Preprocess data\n",
        "    X, y = preprocess_data(df)\n",
        "\n",
        "    # Step 4: Build and train model\n",
        "    model, preprocessor, model_name = build_model(X, y)\n",
        "\n",
        "    # Step 5: Evaluate model\n",
        "    evaluate_model(model, X, y)\n",
        "\n",
        "    # Step 6: Save model\n",
        "    model_filename, preprocessor_filename = save_model(model, preprocessor, model_name)\n",
        "\n",
        "    # Step 7: Create inference function\n",
        "    predict_function = create_inference_function(model_filename, preprocessor_filename)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    duration = end_time - start_time\n",
        "    print(f\"\\nEthiopian Disease Prediction Pipeline completed successfully!\")\n",
        "    print(f\"Total execution time: {duration}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M4zfqzDeyMV",
        "outputId": "3a230854-d84c-4119-f1fd-dbd84a01fccd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Ethiopian Disease Prediction Pipeline at 2025-04-21 09:45:06.241846\n",
            "=============================================\n",
            "Generating 2500 synthetic patient records...\n",
            "Generated 2500 patient records with 20 symptoms\n",
            "Performing exploratory data analysis...\n",
            "\n",
            "Basic statistics (numeric features):\n",
            "                         count       mean        std        min        25%  \\\n",
            "age                     2500.0  49.678400  29.632776   0.000000  24.000000   \n",
            "symptom_duration_days   2500.0  15.453200   8.677484   1.000000   8.000000   \n",
            "disease_severity_score  2500.0  93.152498   9.526568  47.920038  87.646051   \n",
            "\n",
            "                          50%    75%    max  \n",
            "age                      49.0   75.0  100.0  \n",
            "symptom_duration_days    16.0   23.0   30.0  \n",
            "disease_severity_score  100.0  100.0  100.0  \n",
            "\n",
            "Disease distribution:\n",
            "diagnosed_disease\n",
            "Tuberculosis                   354\n",
            "Hepatitis                      316\n",
            "Intestinal Parasites           306\n",
            "Pneumonia                      290\n",
            "HIV/AIDS                       273\n",
            "Malaria                        264\n",
            "Acute Respiratory Infection    254\n",
            "Typhoid Fever                  201\n",
            "Diarrheal Disease              187\n",
            "Meningitis                      55\n",
            "Name: count, dtype: int64\n",
            "EDA complete. Visualizations saved as PNG files.\n",
            "Preprocessing data...\n",
            "Warning: Duplicated columns were removed from the feature set\n",
            "Building and training model...\n",
            "\n",
            "Feature columns: ['gender', 'region', 'severity_level', 'age', 'symptom_duration_days', 'condition_count', 'symptom_count', 'symptom_density', 'age_severity_interaction', 'symptom_fever', 'symptom_cough', 'symptom_headache', 'symptom_fatigue', 'symptom_nausea', 'symptom_vomiting', 'symptom_diarrhea', 'symptom_abdominal_pain', 'symptom_chest_pain', 'symptom_difficulty_breathing', 'symptom_joint_pain', 'symptom_rash', 'symptom_sore_throat', 'symptom_chills', 'symptom_loss_of_appetite', 'symptom_jaundice', 'symptom_swelling', 'symptom_blood_in_stool', 'symptom_night_sweats', 'symptom_weight_loss']\n",
            "Number of features: 29\n",
            "Any duplicate columns: False\n",
            "\n",
            "Training RandomForest...\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "RandomForest - Best parameters: {'model__max_depth': 20, 'model__min_samples_split': 2, 'model__n_estimators': 100}\n",
            "RandomForest - R² Score: 0.6397\n",
            "RandomForest - MAE: 3.9503\n",
            "RandomForest - RMSE: 5.9123\n",
            "\n",
            "Training GradientBoosting...\n",
            "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
            "GradientBoosting - Best parameters: {'model__learning_rate': 0.1, 'model__n_estimators': 100}\n",
            "GradientBoosting - R² Score: 0.6731\n",
            "GradientBoosting - MAE: 3.8038\n",
            "GradientBoosting - RMSE: 5.6316\n",
            "\n",
            "Training XGBoost...\n",
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "XGBoost - Best parameters: {'model__learning_rate': 0.05, 'model__max_depth': 5, 'model__n_estimators': 100}\n",
            "XGBoost - R² Score: 0.6603\n",
            "XGBoost - MAE: 3.8066\n",
            "XGBoost - RMSE: 5.7406\n",
            "\n",
            "Best model: GradientBoosting with R² score of 0.6731\n",
            "Cross-validation R² scores: [0.70688684 0.63142396 0.67087179 0.65961191 0.668951  ]\n",
            "Mean CV R² score: 0.6675\n",
            "\n",
            "Evaluating model performance...\n",
            "R² Score: 0.7414\n",
            "Mean Absolute Error: 3.3318\n",
            "Root Mean Squared Error: 4.8435\n",
            "Accuracy (predictions within ±5 points): 0.7520 (75.20%)\n",
            "Accuracy (predictions within ±10 points): 0.9340 (93.40%)\n",
            "\n",
            "Top 10 important features:\n",
            "                  feature  importance\n",
            "23          symptom_count    0.394984\n",
            "3           symptom_count    0.365056\n",
            "0                     age    0.038810\n",
            "31       symptom_diarrhea    0.038119\n",
            "2         condition_count    0.031493\n",
            "22  symptom_duration_days    0.015029\n",
            "43   symptom_night_sweats    0.011815\n",
            "4         symptom_density    0.010764\n",
            "33     symptom_chest_pain    0.010743\n",
            "1   symptom_duration_days    0.010506\n",
            "Model evaluation complete. Visualizations saved as PNG files.\n",
            "\n",
            "Saving model and preprocessor...\n",
            "Model saved as: ethiopian_disease_model_GradientBoosting_20250421_094544.pkl\n",
            "Preprocessor saved as: ethiopian_disease_preprocessor_20250421_094544.pkl\n",
            "\n",
            "Inference function created successfully.\n",
            "\n",
            "Example inference usage:\n",
            "Example Patient - Predicted Severity Score: 84.52\n",
            "Example Patient - Risk Level: Severe\n",
            "\n",
            "Ethiopian Disease Prediction Pipeline completed successfully!\n",
            "Total execution time: 0:00:38.756497\n"
          ]
        }
      ]
    }
  ]
}